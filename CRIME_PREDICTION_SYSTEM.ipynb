{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzKYMLjT3niD0Cag+Oc6XI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Promise-fachi/cyb3rf4ch1/blob/main/CRIME_PREDICTION_SYSTEM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc3AUNRvpWtO",
        "outputId": "3eb0a211-2266-4fb3-f162-c80a04d92958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully changed working directory to: /content/drive/My Drive/crime_prediction\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your 'crime prediction' folder in Google Drive\n",
        "# Correcting the path to use 'crime_prediction' (with an underscore) as seen in !ls output\n",
        "crime_prediction_folder_path = '/content/drive/My Drive/crime_prediction'\n",
        "\n",
        "# Check if the folder exists before changing directory\n",
        "if os.path.exists(crime_prediction_folder_path):\n",
        "    os.chdir(crime_prediction_folder_path)\n",
        "    print(f\"Successfully changed working directory to: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"Error: The folder '{crime_prediction_folder_path}' does not exist. Please check the folder name and path.\")\n",
        "    print(\"You can list your Google Drive contents with `!ls '/content/drive/My Drive'` to confirm the correct folder name.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5cfa42d",
        "outputId": "e365c7b8-c7c7-413e-96a5-989234fcae61"
      },
      "source": [
        "!ls '/content/drive/My Drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\n",
            " crime_prediction\n",
            "'DIPHEN_K_HR[1] (1).gdoc'\n",
            "'DIPHEN_K_HR[1].gdoc'\n",
            "'DIPHEN_K_HR[1].pdf'\n",
            " FACHIWEB.html\n",
            "'HCI UNIT 4 ASSIGNMENT 1.docx'\n",
            "'HCI UNIT 5 ASSIGNMENT 1.docx'\n",
            "'human computer interaction promise.docx'\n",
            "'IS MONEY BAD.gform'\n",
            "'MARKETING RESEARCH.gform'\n",
            "'MARKETING RESEARCH (Responses).gsheet'\n",
            "'OFFER LETTER.pdf'\n",
            "'Promise Ndhlovu Resume (1).docx'\n",
            "'Promise Ndhlovu Resume.docx'\n",
            "'RESUME PROMISE .pdf'\n",
            "'The impact of digital marketing on consumer purchasing behavior. .gform'\n",
            "'The impact of digital marketing on consumer purchasing behavior.  (Responses).gsheet'\n",
            "'THE ROLE OF INFLUENCER MARKETING IN SHAPING CONSUMER PERCEPTION OF BRAND ADVOCACY.gform'\n",
            "'THE ROLE OF INFLUENCER MARKETING IN SHAPING CONSUMER PERCEPTION OF BRAND ADVOCACY (Responses).gsheet'\n",
            "'Untitled form (1).gform'\n",
            "'Untitled form (2).gform'\n",
            "'Untitled form.gform'\n",
            "'Zedbounty writeup.docx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4e52b67",
        "outputId": "1459b69b-fa6d-4260-d907-7c3a8e537dda"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define the path to your 'crime prediction' folder in Google Drive\n",
        "# Correcting the path to use 'crime_prediction' (with an underscore) as seen in !ls output\n",
        "crime_prediction_folder_path = '/content/drive/My Drive/crime_prediction'\n",
        "\n",
        "# Check if the folder exists before changing directory\n",
        "if os.path.exists(crime_prediction_folder_path):\n",
        "    os.chdir(crime_prediction_folder_path)\n",
        "    print(f\"Successfully changed working directory to: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"Error: The folder '{crime_prediction_folder_path}' does not exist. Please check the folder name and path.\")\n",
        "    print(\"You can list your Google Drive contents with `!ls '/content/drive/My Drive'` to confirm the correct folder name.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully changed working directory to: /content/drive/My Drive/crime_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "445d9ece",
        "outputId": "4a50fdcf-840e-46db-b54b-eaf1748d0512"
      },
      "source": [
        "# Verify the current working directory\n",
        "!pwd"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/crime_prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74124be3"
      },
      "source": [
        "Now you are working directly within your `crime prediction` folder. You can use `!ls` to see its contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b6bd011",
        "outputId": "78c09279-09b5-4fb0-a173-8dcb79ff28ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6f17da3"
      },
      "source": [
        "# Task\n",
        "The environment has been successfully configured, and we are ready for you to provide the next specific task or code for review and development within the `crime_prediction` workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11596edc"
      },
      "source": [
        "## Verify Existing Code Status\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the previously executed code for mounting Google Drive and changing the working directory to `crime_prediction` is functioning correctly and without errors. This step serves as a foundational check for subsequent operations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7f0aee5"
      },
      "source": [
        "## Prepare for New Code Review\n",
        "\n",
        "### Subtask:\n",
        "Indicate readiness to assist with reviewing and correcting any new code or scripts the user wishes to develop within the established `crime_prediction` workspace. This includes identifying and rectifying logical and programming errors to ensure smooth execution and integration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ceac73"
      },
      "source": [
        "### Ready for Code Review\n",
        "\n",
        "I am now ready to assist with reviewing and correcting any new code or scripts you wish to develop within the `crime_prediction` workspace. Please provide the code, and I will help identify and rectify logical and programming errors to ensure smooth execution and integration. You can also specify any new tasks or objectives you'd like to pursue within this directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3239c1d4"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm the environment is ready for coding and await the user's next specific task or code submission.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72f72311"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Google Drive was successfully mounted, and the working directory was correctly changed to `/content/drive/My Drive/crime_prediction`.\n",
        "*   The system is prepared to assist with reviewing and correcting new code or scripts within the established `crime_prediction` workspace.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The environment is now fully configured and ready for development.\n",
        "*   The system is awaiting new specific tasks or code submissions for review and integration within the `crime_prediction` workspace.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4150a0a"
      },
      "source": [
        "# Task\n",
        "Define a data strategy and develop data handling utilities for your crime prediction project, specifying how primary and intermediate data will be loaded, processed, and stored within the `/content/drive/My Drive/crime_prediction` directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90df45d7"
      },
      "source": [
        "## Define Data Strategy\n",
        "\n",
        "### Subtask:\n",
        "Outline how primary and intermediate data will be loaded, processed, and potentially saved/reloaded to ensure consistency and accessibility across different code segments or files within your `crime_prediction` directory. This might involve using common data formats like CSV, Parquet, or Pickle for persistent storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06adbe01"
      },
      "source": [
        "```markdown\n",
        "## Data Strategy for Crime Prediction Project\n",
        "\n",
        "To ensure consistency, accessibility, and efficient workflow within the `/content/drive/My Drive/crime_prediction` directory, the following data strategy will be implemented:\n",
        "\n",
        "### 1. Data Types and Storage Formats:\n",
        "\n",
        "*   **Raw Input Data:** Initial datasets, likely in `.csv` format, will be stored directly in a `data/raw` subdirectory within `crime_prediction`. CSV is chosen for its widespread compatibility and ease of initial inspection.\n",
        "*   **Processed/Cleaned Data:** After initial loading, cleaning, and preprocessing, intermediate datasets will be saved. For tabular data, `.parquet` format will be preferred due to its efficiency in storage and I/O performance for larger datasets. For smaller, simpler intermediate results, `.csv` might still be used. These will be stored in `data/processed`.\n",
        "*   **Engineered Features:** Datasets containing newly engineered features will also be stored in `.parquet` format within `data/features` to maintain a distinct separation from raw and processed data.\n",
        "*   **Model Outputs/Results:** Trained models (e.g., using `pickle` or `joblib` for Python objects) and their evaluation metrics will be saved in a `models/` directory. Prediction outputs will be saved as `.csv` or `.parquet` in `results/`.\n",
        "\n",
        "### 2. Initial Data Loading:\n",
        "\n",
        "*   Raw data files (e.g., `crime_data.csv`) will be loaded using `pandas` from their respective `data/raw` subdirectories. The `os.chdir` command ensures the script operates from the `crime_prediction` root, simplifying relative pathing.\n",
        "    ```python\n",
        "    import pandas as pd\n",
        "    raw_df = pd.read_csv('data/raw/crime_data.csv')\n",
        "    ```\n",
        "\n",
        "### 3. Processing, Saving, and Naming Intermediate Data:\n",
        "\n",
        "*   **Processing:** Data will be processed using `pandas` for cleaning, transformation, and feature engineering. Functions will be encapsulated to promote reusability and modularity.\n",
        "*   **Saving:** Intermediate data will be saved to their designated subdirectories (`data/processed`, `data/features`).\n",
        "    *   **File Naming Convention:** Files will be named descriptively, incorporating the processing step and a version or timestamp if necessary (e.g., `processed_crime_data_v1.parquet`, `engineered_features_20231027.parquet`).\n",
        "    ```python\n",
        "    # Example of saving processed data\n",
        "    processed_df.to_parquet('data/processed/processed_crime_data_v1.parquet', index=False)\n",
        "\n",
        "    # Example of saving engineered features\n",
        "    features_df.to_parquet('data/features/engineered_features_v1.parquet', index=False)\n",
        "    ```\n",
        "\n",
        "### 4. Reloading Intermediate Data:\n",
        "\n",
        "*   Saved intermediate data can be easily reloaded in subsequent scripts or notebook sessions, avoiding the need to re-run time-consuming preprocessing steps.\n",
        "    ```python\n",
        "    import pandas as pd\n",
        "    reloaded_processed_df = pd.read_parquet('data/processed/processed_crime_data_v1.parquet')\n",
        "    reloaded_features_df = pd.read_parquet('data/features/engineered_features_v1.parquet')\n",
        "    ```\n",
        "\n",
        "### Directory Structure:\n",
        "\n",
        "```\n",
        "crime_prediction/\n",
        "├── data/\n",
        "│   ├── raw/\n",
        "│   │   └── crime_data.csv\n",
        "│   ├── processed/\n",
        "│   │   └── processed_crime_data_v1.parquet\n",
        "│   └── features/\n",
        "│       └── engineered_features_v1.parquet\n",
        "├── models/\n",
        "│   └── trained_model_v1.pkl\n",
        "├── results/\n",
        "│   └── predictions_20231027.csv\n",
        "├── notebooks/\n",
        "│   └── 01_data_preprocessing.ipynb\n",
        "│   └── 02_feature_engineering.ipynb\n",
        "│   └── 03_model_training.ipynb\n",
        "└── app/\n",
        "    └── ... (Flask app files)\n",
        "```\n",
        "\n",
        "This structured approach ensures a clear data lineage, simplifies collaboration, and optimizes development by allowing specific stages of the project to be run or re-run independently.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705bd5d1"
      },
      "source": [
        "## Develop Data Handling Utilities\n",
        "\n",
        "### Subtask:\n",
        "Create reusable utility functions or classes for loading and saving datasets, centralizing data access, and ensuring that all parts of your project consistently retrieve and store the correct version of the shared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5348fea7"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will create the necessary directory structure for raw, processed, and features data to ensure that subsequent file operations do not fail due to missing directories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e8932fa",
        "outputId": "e38ef901-d308-486d-d1d4-5a099cd0b835"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define base data directory\n",
        "data_base_path = 'data'\n",
        "raw_data_path = os.path.join(data_base_path, 'raw')\n",
        "processed_data_path = os.path.join(data_base_path, 'processed')\n",
        "features_data_path = os.path.join(data_base_path, 'features')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(raw_data_path, exist_ok=True)\n",
        "os.makedirs(processed_data_path, exist_ok=True)\n",
        "os.makedirs(features_data_path, exist_ok=True)\n",
        "\n",
        "print(f\"Created data directories: {raw_data_path}, {processed_data_path}, {features_data_path}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created data directories: data/raw, data/processed, data/features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62018298"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a set of utility functions for loading and saving data to the respective 'raw', 'processed', and 'features' directories using pandas, ensuring robust file path handling and basic error management as requested by the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68cb5c35",
        "outputId": "b1660a4c-d9da-45bc-9d44-e91775ce11bc"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def load_raw_data(filename):\n",
        "    \"\"\"Loads a CSV file from the data/raw directory.\"\"\"\n",
        "    filepath = os.path.join('data', 'raw', filename)\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Successfully loaded raw data from: {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Raw data file not found at {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading raw data from {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_processed_data(df, filename):\n",
        "    \"\"\"Saves a DataFrame as a Parquet file to the data/processed directory.\"\"\"\n",
        "    filepath = os.path.join('data', 'processed', filename)\n",
        "    try:\n",
        "        df.to_parquet(filepath, index=False)\n",
        "        print(f\"Successfully saved processed data to: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving processed data to {filepath}: {e}\")\n",
        "\n",
        "def load_processed_data(filename):\n",
        "    \"\"\"Loads a Parquet file from the data/processed directory.\"\"\"\n",
        "    filepath = os.path.join('data', 'processed', filename)\n",
        "    try:\n",
        "        df = pd.read_parquet(filepath)\n",
        "        print(f\"Successfully loaded processed data from: {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Processed data file not found at {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading processed data from {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_features_data(df, filename):\n",
        "    \"\"\"Saves a DataFrame as a Parquet file to the data/features directory.\"\"\"\n",
        "    filepath = os.path.join('data', 'features', filename)\n",
        "    try:\n",
        "        df.to_parquet(filepath, index=False)\n",
        "        print(f\"Successfully saved features data to: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving features data to {filepath}: {e}\")\n",
        "\n",
        "def load_features_data(filename):\n",
        "    \"\"\"Loads a Parquet file from the data/features directory.\"\"\"\n",
        "    filepath = os.path.join('data', 'features', filename)\n",
        "    try:\n",
        "        df = pd.read_parquet(filepath)\n",
        "        print(f\"Successfully loaded features data from: {filepath}\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Features data file not found at {filepath}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading features data from {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Data handling utility functions defined: load_raw_data, save_processed_data, load_processed_data, save_features_data, load_features_data\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data handling utility functions defined: load_raw_data, save_processed_data, load_processed_data, save_features_data, load_features_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52e12dd0"
      },
      "source": [
        "## Structure Code for Integration\n",
        "\n",
        "### Subtask:\n",
        "Advise on organizing your upcoming code into logical functions or classes. This modular approach will clarify data flow between different processing steps and make it easier to ensure that data is passed and used correctly across all components of your system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "604cd059"
      },
      "source": [
        "```markdown\n",
        "## Structuring Code for Integration: A Modular Approach\n",
        "\n",
        "To ensure clarity, maintainability, and efficient data flow within your crime prediction project, a modular coding approach is highly recommended. This involves breaking down the project into distinct, manageable components.\n",
        "\n",
        "### 1. Modular Project Structure with Distinct Python Modules\n",
        "\n",
        "Organizing your code into separate Python modules enhances readability, reusability, and collaboration. Each module should have a single, clear responsibility.\n",
        "\n",
        "*   **`data_preprocessing.py`**: Contains functions for cleaning raw data, handling missing values, standardizing formats, and any initial transformations.\n",
        "*   **`feature_engineering.py`**: Houses functions dedicated to creating new features from existing data, applying transformations to improve model performance, and selecting relevant features.\n",
        "*   **`model_training.py`**: Encompasses functions for training machine learning models, hyperparameter tuning, and evaluating model performance.\n",
        "*   **`prediction.py`**: Contains functions for making predictions using trained models and potentially post-processing predictions.\n",
        "*   **`utils.py`**: A general utility module for common helper functions that might be used across multiple parts of the project (e.g., custom metrics, specialized data loaders/savers beyond the core utilities).\n",
        "*   **`data_utilities.py`**: (This would be where the `load_raw_data`, `save_processed_data`, etc., functions reside, making them easily importable.)\n",
        "\n",
        "### 2. Encapsulate Functionality in Well-Defined Functions\n",
        "\n",
        "Each major step in your workflow should be an individual function with a clear purpose, inputs, and outputs. This makes debugging easier and allows for testing individual components.\n",
        "\n",
        "*   **Cleaning Example:**\n",
        "    ```python\n",
        "    # In data_preprocessing.py\n",
        "    def clean_crime_data(raw_df):\n",
        "        # ... cleaning logic ...\n",
        "        return cleaned_df\n",
        "    ```\n",
        "*   **Feature Engineering Example:**\n",
        "    ```python\n",
        "    # In feature_engineering.py\n",
        "    def create_temporal_features(df):\n",
        "        # ... feature creation logic ...\n",
        "        return df_with_new_features\n",
        "    ```\n",
        "*   **Model Training Example:**\n",
        "    ```python\n",
        "    # In model_training.py\n",
        "    def train_model(X_train, y_train, model_params):\n",
        "        # ... model training logic ...\n",
        "        return trained_model\n",
        "    ```\n",
        "\n",
        "### 3. Utilize the Previously Defined Data Utilities\n",
        "\n",
        "The `load_raw_data`, `save_processed_data`, `load_processed_data`, `save_features_data`, and `load_features_data` functions should be central to managing data persistence and flow between these functional blocks. This ensures consistency and avoids redundant processing.\n",
        "\n",
        "```python\n",
        "# Example of integrating data utilities within a workflow\n",
        "from data_utilities import load_raw_data, save_processed_data, load_processed_data, save_features_data\n",
        "from data_preprocessing import clean_crime_data\n",
        "from feature_engineering import create_temporal_features\n",
        "\n",
        "# --- First run: Load raw, process, engineer features, save intermediates ---\n",
        "raw_crime_df = load_raw_data('crime_data.csv')\n",
        "if raw_crime_df is not None:\n",
        "    processed_crime_df = clean_crime_data(raw_crime_df)\n",
        "    save_processed_data(processed_crime_df, 'processed_crime_data_v1.parquet')\n",
        "\n",
        "    # Load processed data (or continue with in-memory df)\n",
        "    processed_crime_df = load_processed_data('processed_crime_data_v1.parquet') # For demonstration, imagine a separate script\n",
        "    if processed_crime_df is not None:\n",
        "        features_df = create_temporal_features(processed_crime_df)\n",
        "        save_features_data(features_df, 'engineered_features_v1.parquet')\n",
        "\n",
        "# --- Subsequent run or another script: Directly load engineered features ---\n",
        "# features_df_reloaded = load_features_data('engineered_features_v1.parquet')\n",
        "# if features_df_reloaded is not None:\n",
        "#     X_train, X_test, y_train, y_test = prepare_for_model(features_df_reloaded)\n",
        "#     trained_model = train_model(X_train, y_train, model_params)\n",
        "```\n",
        "\n",
        "### 4. Consider Using Classes for Complex Components\n",
        "\n",
        "While functions are excellent for individual operations, classes can be beneficial when you need to encapsulate related data and methods, or when state needs to be maintained across multiple operations. This is particularly useful for complex components.\n",
        "\n",
        "*   **`Preprocessor` Class:** Could encapsulate various cleaning and transformation steps that share configurations or a common state (e.g., scalers fit on training data). A `Preprocessor` class could have methods like `fit`, `transform`, and `fit_transform`.\n",
        "    ```python\n",
        "    # In data_preprocessing.py\n",
        "    class CrimePreprocessor:\n",
        "        def __init__(self, config=None):\n",
        "            self.scaler = None # Example state\n",
        "            # ... initialization ...\n",
        "\n",
        "        def fit(self, df):\n",
        "            # Fit scalers, encoders etc. based on df\n",
        "            self.scaler = StandardScaler().fit(df[['numerical_col']])\n",
        "            return self\n",
        "\n",
        "        def transform(self, df):\n",
        "            # Apply transformations\n",
        "            df['numerical_col'] = self.scaler.transform(df[['numerical_col']])\n",
        "            return df\n",
        "\n",
        "        def fit_transform(self, df):\n",
        "            return self.fit(df).transform(df)\n",
        "    ```\n",
        "*   **`ModelTrainer` Class:** Can manage model instantiation, training, evaluation, and saving/loading of the model.\n",
        "\n",
        "### 5. Emphasize Clear Input/Output for Functions/Methods\n",
        "\n",
        "Every function or method should have explicitly defined inputs and outputs. This clarity is crucial for understanding data flow, debugging, and ensuring data consistency. Use type hints for even greater clarity.\n",
        "\n",
        "```python\n",
        "# Clear input (DataFrame) and output (DataFrame)\n",
        "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # ... logic ...\n",
        "    return df\n",
        "```\n",
        "\n",
        "### 6. Suggest a Main Orchestration Script\n",
        "\n",
        "A central script, often named `run_pipeline.py` or `main.py`, should define the overall execution flow of your project. This script imports functions and classes from other modules and orchestrates their execution in the correct sequence.\n",
        "\n",
        "```python\n",
        "# In run_pipeline.py\n",
        "from data_utilities import load_raw_data, save_processed_data, save_features_data\n",
        "from data_preprocessing import CrimePreprocessor\n",
        "from feature_engineering import create_temporal_features\n",
        "from model_training import train_model, evaluate_model\n",
        "import pandas as pd\n",
        "import joblib # for saving the preprocessor and model\n",
        "\n",
        "def main():\n",
        "    # 1. Load Raw Data\n",
        "    raw_df = load_raw_data('crime_data.csv')\n",
        "    if raw_df is None:\n",
        "        print(\"Exiting pipeline due to raw data loading error.\")\n",
        "        return\n",
        "\n",
        "    # 2. Preprocess Data (using a class)\n",
        "    preprocessor = CrimePreprocessor()\n",
        "    processed_df = preprocessor.fit_transform(raw_df.copy())\n",
        "    save_processed_data(processed_df, 'processed_crime_data_v1.parquet')\n",
        "    joblib.dump(preprocessor, 'models/preprocessor.pkl') # Save the fitted preprocessor\n",
        "\n",
        "    # 3. Feature Engineering\n",
        "    features_df = create_temporal_features(processed_df.copy())\n",
        "    save_features_data(features_df, 'engineered_features_v1.parquet')\n",
        "\n",
        "    # 4. Prepare for Modeling (e.g., split data, handle target variable)\n",
        "    # Assuming 'target' is the crime prediction target variable\n",
        "    X = features_df.drop('target', axis=1)\n",
        "    y = features_df['target']\n",
        "    # ... perform train-test split ...\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Train Model\n",
        "    model_params = {'n_estimators': 100, 'random_state': 42}\n",
        "    trained_model = train_model(X_train, y_train, model_params)\n",
        "    joblib.dump(trained_model, 'models/trained_model_v1.pkl')\n",
        "\n",
        "    # 6. Evaluate Model\n",
        "    evaluate_model(trained_model, X_test, y_test)\n",
        "\n",
        "    print(\"Pipeline execution complete.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "```\n",
        "\n",
        "This structured approach makes the project scalable, easier to debug, and simple to collaborate on, while ensuring a clear data flow through all stages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "200fcf94"
      },
      "source": [
        "## Code Review for Data Consistency\n",
        "\n",
        "### Subtask:\n",
        "Provide new code for review to ensure it correctly integrates with the defined data strategy and consistently uses shared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ec4a0e8"
      },
      "source": [
        "**Reasoning**:\n",
        "To demonstrate the defined data strategy and data handling utilities, I will first create a dummy raw CSV file in the 'data/raw' directory. This simulates having initial data to work with.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28dc3b35",
        "outputId": "f2da514e-55ca-439f-8d60-12b0a88c8564"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Create some dummy raw data for demonstration\n",
        "dummy_data = {\n",
        "    'Crime_ID': range(1, 11),\n",
        "    'Crime_Type': ['Theft', 'Assault', 'Burglary', 'Vandalism', 'Theft', 'Assault', 'Robbery', 'Theft', 'Vandalism', 'Burglary'],\n",
        "    'Location': ['Area A', 'Area B', 'Area C', 'Area A', 'Area D', 'Area B', 'Area E', 'Area A', 'Area C', 'Area F'],\n",
        "    'Date': pd.to_datetime(['2023-01-01', '2023-01-05', '2023-01-10', '2023-01-12', '2023-01-15', '2023-01-18', '2023-01-20', '2023-01-22', '2023-01-25', '2023-01-28']),\n",
        "    'Victims': [1, 1, 0, 0, 1, 2, 1, 1, 0, 0],\n",
        "    'Severity': [3, 4, 2, 1, 3, 5, 4, 3, 1, 2]\n",
        "}\n",
        "dummy_df = pd.DataFrame(dummy_data)\n",
        "\n",
        "# Define the path for the dummy raw data file\n",
        "dummy_raw_filepath = os.path.join('data', 'raw', 'dummy_crime_data.csv')\n",
        "\n",
        "# Save the dummy data to the raw data directory\n",
        "dummy_df.to_csv(dummy_raw_filepath, index=False)\n",
        "\n",
        "print(f\"Dummy raw data saved to: {dummy_raw_filepath}\")\n",
        "print(\"Head of the dummy data:\")\n",
        "print(dummy_df.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy raw data saved to: data/raw/dummy_crime_data.csv\n",
            "Head of the dummy data:\n",
            "   Crime_ID Crime_Type Location       Date  Victims  Severity\n",
            "0         1      Theft   Area A 2023-01-01        1         3\n",
            "1         2    Assault   Area B 2023-01-05        1         4\n",
            "2         3   Burglary   Area C 2023-01-10        0         2\n",
            "3         4  Vandalism   Area A 2023-01-12        0         1\n",
            "4         5      Theft   Area D 2023-01-15        1         3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfb9c1f"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the dummy raw data is created and utility functions are defined, I will write the first segment of the new code. This code will load the raw data using the `load_raw_data` utility, perform a simple preprocessing step, and then save the processed data using the `save_processed_data` utility, demonstrating the integration with the defined data strategy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82684c15",
        "outputId": "633489a0-d1bf-44b2-f20f-c6581308149c"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming data handling utility functions are defined in the current scope or can be imported.\n",
        "# For demonstration purposes, we will assume they are available directly, as they were defined in previous cells.\n",
        "\n",
        "# 1. Load raw data using the utility function\n",
        "raw_df = load_raw_data('dummy_crime_data.csv')\n",
        "\n",
        "if raw_df is not None:\n",
        "    print(\"\\nInitial raw data loaded successfully. Performing preprocessing...\")\n",
        "\n",
        "    # 2. Perform a simple preprocessing step:\n",
        "    # Ensure 'Date' column is in datetime format (it was already created as such but demonstrating a common preprocessing step)\n",
        "    raw_df['Date'] = pd.to_datetime(raw_df['Date'])\n",
        "    # Create a simple new feature: 'DayOfWeek'\n",
        "    raw_df['DayOfWeek'] = raw_df['Date'].dt.day_name()\n",
        "    # Convert 'Crime_Type' to category for memory efficiency and potential future use\n",
        "    raw_df['Crime_Type'] = raw_df['Crime_Type'].astype('category')\n",
        "    # Drop original 'Date' column if not needed for further steps, or keep it depending on strategy\n",
        "    # For this example, we'll keep it but show how to drop if necessary:\n",
        "    # processed_df = raw_df.drop(columns=['Date'])\n",
        "    processed_df = raw_df.copy() # Make a copy to avoid modifying the original raw_df in place\n",
        "\n",
        "    print(\"Preprocessing complete. Head of processed data:\")\n",
        "    print(processed_df.head())\n",
        "\n",
        "    # 3. Save processed data using the utility function\n",
        "    processed_filename = 'processed_dummy_crime_data_v1.parquet'\n",
        "    save_processed_data(processed_df, processed_filename)\n",
        "\n",
        "    print(f\"\\nAttempting to reload processed data to verify: {processed_filename}\")\n",
        "    reloaded_processed_df = load_processed_data(processed_filename)\n",
        "\n",
        "    if reloaded_processed_df is not None:\n",
        "        print(\"Successfully reloaded processed data. Head of reloaded data:\")\n",
        "        print(reloaded_processed_df.head())\n",
        "        print(f\"Columns of reloaded processed data: {reloaded_processed_df.columns.tolist()}\")\n",
        "    else:\n",
        "        print(\"Failed to reload processed data.\")\n",
        "else:\n",
        "    print(\"Skipping preprocessing and saving as raw data loading failed.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded raw data from: data/raw/dummy_crime_data.csv\n",
            "\n",
            "Initial raw data loaded successfully. Performing preprocessing...\n",
            "Preprocessing complete. Head of processed data:\n",
            "   Crime_ID Crime_Type Location       Date  Victims  Severity DayOfWeek\n",
            "0         1      Theft   Area A 2023-01-01        1         3    Sunday\n",
            "1         2    Assault   Area B 2023-01-05        1         4  Thursday\n",
            "2         3   Burglary   Area C 2023-01-10        0         2   Tuesday\n",
            "3         4  Vandalism   Area A 2023-01-12        0         1  Thursday\n",
            "4         5      Theft   Area D 2023-01-15        1         3    Sunday\n",
            "Successfully saved processed data to: data/processed/processed_dummy_crime_data_v1.parquet\n",
            "\n",
            "Attempting to reload processed data to verify: processed_dummy_crime_data_v1.parquet\n",
            "Successfully loaded processed data from: data/processed/processed_dummy_crime_data_v1.parquet\n",
            "Successfully reloaded processed data. Head of reloaded data:\n",
            "   Crime_ID Crime_Type Location       Date  Victims  Severity DayOfWeek\n",
            "0         1      Theft   Area A 2023-01-01        1         3    Sunday\n",
            "1         2    Assault   Area B 2023-01-05        1         4  Thursday\n",
            "2         3   Burglary   Area C 2023-01-10        0         2   Tuesday\n",
            "3         4  Vandalism   Area A 2023-01-12        0         1  Thursday\n",
            "4         5      Theft   Area D 2023-01-15        1         3    Sunday\n",
            "Columns of reloaded processed data: ['Crime_ID', 'Crime_Type', 'Location', 'Date', 'Victims', 'Severity', 'DayOfWeek']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ffb87e4"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Confirm that the integrated system is functioning correctly, and data is consistently shared and managed across all code components within the `crime_prediction` workspace.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bfa1986"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Comprehensive Data Strategy Established:** A detailed data strategy was defined, outlining specific storage formats and locations for raw data (CSV in `data/raw`), processed/cleaned data (Parquet in `data/processed`), and engineered features (Parquet in `data/features`). Models will be stored in `models/` (pickle/joblib), and predictions in `results/` (CSV/Parquet).\n",
        "*   **Reusable Data Handling Utilities Developed:** Dedicated Python functions were created for centralizing data access. These include `load_raw_data` (for CSV), `save_processed_data`, `load_processed_data`, `save_features_data`, and `load_features_data` (all for Parquet). These utilities incorporate robust error handling and proper path management.\n",
        "*   **Modular Code Structure Advised:** A modular approach was recommended for organizing code into distinct Python modules (e.g., `data_preprocessing.py`, `feature_engineering.py`, `data_utilities.py`), emphasizing well-defined functions and the integration of the developed data utilities. The use of classes for complex components and a main orchestration script (`run_pipeline.py`) was also suggested.\n",
        "*   **Data Strategy and Utilities Successfully Integrated and Verified:** A practical demonstration successfully showcased the integration of the defined data strategy and utilities. Dummy raw data was loaded using `load_raw_data`, processed (including feature engineering), saved as Parquet using `save_processed_data`, and then reloaded using `load_processed_data`, confirming consistent data flow and successful persistence according to the established strategy.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The established data strategy and developed utilities provide a strong foundation for managing data consistency and accessibility throughout the `crime_prediction` project, significantly improving maintainability and collaboration.\n",
        "*   The next logical step is to implement the advised modular code structure by creating the specified Python modules (e.g., `data_preprocessing.py`, `feature_engineering.py`) and incorporating the `data_utilities` functions to build out the full data processing and model training pipeline.\n"
      ]
    }
  ]
}